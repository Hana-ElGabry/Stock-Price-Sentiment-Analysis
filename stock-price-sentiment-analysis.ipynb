{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"üß† Stock Price Sentiment Analysis using ML & Deep Learning (LSTM)\n\nThis notebook demonstrates sentiment analysis on stock-related text data using:\n1. **Machine Learning Approach**: TF-IDF + Logistic Regression\n2. **Deep Learning Approach**: LSTM Neural Network\n\nWe'll compare the performance of both methods.","metadata":{"_uuid":"f76f626b-f6a5-495d-82ae-c12a7a6574ac","_cell_guid":"7a8e2dc7-6d18-4754-9f2b-b01215bce4e1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## üì¶ Step 1: Load Dataset and Dependencies","metadata":{"_uuid":"a23f41eb-c4c8-4df1-a6c3-695daa941e5a","_cell_guid":"5fee2215-6fe1-4f79-b228-54eef5dd9fe8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n\n# Load data\ndf = pd.read_csv('/kaggle/input/stock-sentiment-data/stock_sentiment.csv')\nprint(\"\\nData Sample:\")\nprint(df.head())\n\n# Filter columns\ndf = df[['text', 'sentiment']].dropna()","metadata":{"_uuid":"90ae58e6-ed26-447d-9b87-0249a546bd21","_cell_guid":"eb0505da-8430-4c0d-bd54-961b278f1be0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-23T15:15:36.527432Z","iopub.execute_input":"2025-05-23T15:15:36.527682Z","iopub.status.idle":"2025-05-23T15:15:36.924711Z","shell.execute_reply.started":"2025-05-23T15:15:36.527662Z","shell.execute_reply":"2025-05-23T15:15:36.922882Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üìÉ Step 2: Text Preprocessing","metadata":{"_uuid":"d14b720b-7fbc-490a-a320-d4c2cdae1862","_cell_guid":"a720d64e-b671-44c9-87e4-3d28ce8ca711","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def clean_text(text):\n    text = text.lower()\n    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n    text = re.sub(r'\\@\\w+|\\#', '', text)\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    text = re.sub(r'\\d+', '', text)\n    return text.strip()\n\ndf['clean_text'] = df['text'].apply(clean_text)\ndf['label'] = df['sentiment'].map({'negative': 0, 'neutral': 1, 'positive': 2})","metadata":{"_uuid":"5b5be336-7962-4e27-8b55-2e2b1af7e660","_cell_guid":"cdd48fb0-6068-4c7c-a412-4130592acc81","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-23T15:15:36.926008Z","iopub.execute_input":"2025-05-23T15:15:36.926342Z","iopub.status.idle":"2025-05-23T15:15:37.174897Z","shell.execute_reply.started":"2025-05-23T15:15:36.926322Z","shell.execute_reply":"2025-05-23T15:15:37.173824Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üìä Step 3: Machine Learning Approach (TF-IDF + Logistic Regression)","metadata":{"_uuid":"c346f6ad-78c2-4db6-bee2-e1adfc7cb200","_cell_guid":"9c1a7157-c6b8-4b68-b18b-290bc4eebe77","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# TF-IDF Vectorization\nvectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\nX_tfidf = vectorizer.fit_transform(df['clean_text'])\ny = df['label']\n\n# Train-test split\nX_train_ml, X_test_ml, y_train_ml, y_test_ml = train_test_split(\n    X_tfidf, y, test_size=0.2, random_state=42)\n\n# Train model\nlogreg = LogisticRegression(max_iter=200)\nlogreg.fit(X_train_ml, y_train_ml)\n\n# Evaluate\ny_pred_ml = logreg.predict(X_test_ml)\nprint(\"\\nüîç Logistic Regression Accuracy:\", accuracy_score(y_test_ml, y_pred_ml))\nprint(classification_report(y_test_ml, y_pred_ml))\n\n# Confusion matrix\nplt.figure(figsize=(6,4))\nsns.heatmap(confusion_matrix(y_test_ml, y_pred_ml), \n            annot=True, cmap='Blues', fmt='d',\n            xticklabels=['Neg', 'Neutral', 'Pos'],\n            yticklabels=['Neg', 'Neutral', 'Pos'])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.title(\"Logistic Regression Confusion Matrix\")\nplt.show()","metadata":{"_uuid":"c16c2152-4779-442b-830c-5a18f38b2aa8","_cell_guid":"3c9a9414-e163-4c25-a790-20be7aa06916","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ü§ñ Step 4: Deep Learning Approach (LSTM)","metadata":{"_uuid":"4b7e5891-7f87-4175-b02d-105483252e24","_cell_guid":"a5d4fa3e-3d6d-4633-b04b-8dba3f7765fa","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Tokenization\nmax_words = 10000\nmax_len = 100\n\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(df['clean_text'])\n\nX_seq = tokenizer.texts_to_sequences(df['clean_text'])\nX_pad = pad_sequences(X_seq, maxlen=max_len)\n\n# Split data\ny = df['label']\nX_train_dl, X_test_dl, y_train_dl, y_test_dl = train_test_split(\n    X_pad, y, test_size=0.2, random_state=42)\n\n# Build model\nmodel = Sequential([\n    Embedding(input_dim=max_words, output_dim=128, input_length=max_len),\n    LSTM(64, return_sequences=False),\n    Dropout(0.5),\n    Dense(3, activation='softmax')\n])\n\n# Compile and train\nmodel.compile(loss='sparse_categorical_crossentropy', \n              optimizer='adam', \n              metrics=['accuracy'])\n\nhistory = model.fit(X_train_dl, y_train_dl, \n                   epochs=5, \n                   batch_size=64, \n                   validation_data=(X_test_dl, y_test_dl))\n\n# Evaluate\nloss, acc = model.evaluate(X_test_dl, y_test_dl)\nprint(\"\\nüìà LSTM Test Accuracy:\", acc)\n\n# Plot training\nplt.figure(figsize=(8,4))\nplt.plot(history.history['accuracy'], label='Train Acc')\nplt.plot(history.history['val_accuracy'], label='Val Acc')\nplt.title(\"LSTM Accuracy Over Epochs\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"_uuid":"c07da345-6fc9-4cd8-8cc3-c03deceaca00","_cell_guid":"b0dc41b4-f12d-4680-a4b1-b007d76bb488","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üèÅ Conclusion\n\n**Comparison**:\n- Logistic Regression: Faster training, good baseline\n- LSTM: Better for sequence data, higher potential accuracy\n\n**Next Steps**:\n- Try transformer models (BERT, etc.)\n- Hyperparameter tuning\n- Model deployment","metadata":{"_uuid":"4b24de8c-bb6a-49a2-9cc4-ee57a236e082","_cell_guid":"c4e180b7-4bc9-468e-a3d2-5e998191d681","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}}]}